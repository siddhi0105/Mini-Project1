# -*- coding: utf-8 -*-
"""AL/ML Handwritten digit recognition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gzwTE2IelNxmMZRXHASXou_KrhvAKuwb
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

!pip install ipycanvas ipywidgets

from ipycanvas import Canvas
from ipywidgets import Button, VBox
from IPython.display import display
import ipywidgets as widgets
from IPython.display import display

digit_input = widgets.Text(
    value='',
    placeholder='Enter a digit (0-9)',
    description='Digit:',
    disabled=False
)

recognize_btn = widgets.Button(
    description='Recognize Digit',
    button_style='success'
)

output = widgets.Output()

def recognize_digit(b):
    with output:
        output.clear_output()
        digit = digit_input.value

        if digit.isdigit() and 0 <= int(digit) <= 9:
            print("Recognized Digit:", digit)
        else:
            print("Please enter a valid digit (0â€“9)")

recognize_btn.on_click(recognize_digit)
display(widgets.VBox([digit_input, recognize_btn, output]))

recognize_btn.on_click(recognize_digit)

import pandas as pd

df = pd.read_csv("/content/sample_data/sample_submission.csv")
print(df.head())


train = pd.read_csv('/content/sample_data/train.csv.zip')
test = pd.read_csv('/content/sample_data/test.csv.zip')

print(train.shape)
train.head()

print(test.shape)
test.head()

import matplotlib.pyplot as plt
plt.hist(train["label"])
plt.title("Frequency Histogram of Numbers in Training Data")
plt.xlabel("Number Value")
plt.ylabel("Frequency")
plt.show()

label_train=train['label']
train=train.drop('label', axis=1)

train.head()

train = train/255
test = test/255

from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(train, label_train, train_size = 0.8,random_state = 42)

from sklearn import decomposition

## PCA decomposition
pca = decomposition.PCA(n_components=200) #Finds first 200 PCs
pca.fit(X_train)
plt.plot(pca.explained_variance_ratio_)
plt.ylabel('% of variance explained')
plt.show()

#plot reaches asymptote at around 100, which is optimal number of PCs to use.

## PCA decomposition with optimal number of PCs
#decompose train data
pca = decomposition.PCA(n_components=100)
pca.fit(X_train)
PCtrain = pca.transform(X_train)
PCval = pca.transform(X_val)

#decompose test data
PCtest = pca.transform(test)

X_train= PCtrain

X_cv = PCval

from sklearn.svm import SVC
clf = SVC()
clf.fit(X_train,y_train)

predicted = clf.predict(X_cv)
expected = y_val

print(predicted[0:30])

# Commented out IPython magic to ensure Python compatibility.
from sklearn import  metrics
print("Classification report for classifier %s:\n%s\n"
#       % (clf, metrics.classification_report(expected, predicted)))

print('accurcy :',metrics.accuracy_score(expected, predicted))

output_label = clf.predict(PCtest)

output = pd.DataFrame(output_label,columns = ['Label'])
output.reset_index(inplace=True)
output['index'] = output['index'] + 1
output.rename(columns={'index': 'ImageId'}, inplace=True)
output.to_csv('output.csv', index=False)
output.head()
